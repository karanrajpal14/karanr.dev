---
title: Rating Predictor for BoardGameGeek reviews
date: 2020-05-11
published: true
type: post
tags: [python]
---

# Predict rating of review using BoardGameGeek Reviews dataset

**The goal of this project is to use the corpus of reviews present in this dataset, learn the reviews and their corresponding rating.**

**Once the model is trained using the review data, we ask the user to input a new review and predict the rating of that review.**

We begin by importing all the basic libraries:


```python
!pip install -q pandas numpy nltk scikit-learn matplotlib seaborn wordcloud ipython
```


```python
import random
import gc
import re
import string
import pickle
import os
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import numpy as np
import pandas as pd
from IPython.display import display, HTML

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split, KFold
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.feature_extraction.text import TfidfVectorizer

import seaborn as sns
from matplotlib import pyplot as plt

import nltk
from nltk.corpus import stopwords
nltk.download(['words', 'stopwords', 'wordnet'], quiet=True)
```




    True




```python
from google.colab import drive
drive.mount('/content/drive')
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).


Here we define the paths of the dataset and the pickle files

We will be pickling our dataset after every major step due to save time

Due to the sheer amount of data we have, it does take time to do pre-processing

Also, I will be invoking the garbage collector often just to save memory as we need every bit of it due to the volume of input data


```python
DATA_DIR = "/content/drive/My Drive/Colab Notebooks/data/"

DATASET = DATA_DIR + "bgg-13m-reviews.csv"

REVIEWS_PICKLE = DATA_DIR + "reviews.pkl"
REVIEWS_DATA_CHANGED = False

CLEAN_TEXT_PICKLE = DATA_DIR + "clean_text.pkl"
LEMMATIZED_PICKLE = DATA_DIR + "lemmatized.pkl"
STOPWORDS_PICKLE = DATA_DIR + "stopwords.pkl"
CLEANED_PICKLE = DATA_DIR + "cleaned_reviews.pkl"

MODEL_PICKLE = DATA_DIR + "model.pkl"
VOCABULARY_PICKLE = DATA_DIR + "vocab.pkl"
```

## Loading the data

Now that we've got the formalities out of our way, let's start exploring our dataset and see how we can solve this problem.

First, we need to load the dataset like so:


```python
reviews = pd.DataFrame()

if os.path.isfile(REVIEWS_PICKLE) or REVIEWS_DATA_CHANGED:
    print('Pickled reviews found. Loading from pickle file.')
    reviews = pd.read_pickle(REVIEWS_PICKLE, compression="gzip")
else:
    print("Reading reviews csv")
    reviews = pd.read_csv(DATASET, usecols=['comment', 'rating'])
    print("Pickling dataset")
    reviews.to_pickle(REVIEWS_PICKLE, compression="gzip")

reviews = reviews.reindex(columns=['comment', 'rating'])

reviews = reviews.sample(frac=1, random_state=3).reset_index(drop=True)

gc.collect()
display(reviews)
```

    Pickled reviews found. Loading from pickle file.

|          | comment                                               | rating |
|----------|-------------------------------------------------------|--------|
| 0        | NaN                                                   | 6\.5   |
| 1        | NaN                                                   | 8\.0   |
| 2        | NaN                                                   | 8\.0   |
| 3        | NaN                                                   | 7\.0   |
| 4        | http://boardgamers\.ro/saboteur\-recenzie/            | 6\.0   |
| \.\.\.   | \.\.\.                                                | \.\.\. |
| 13170068 | "Solid deck\-builder in which you need to strat\.\.\. | 6\.5   |
| 13170069 | I adore this game\. Not only is it an incredibl\.\.\. | 10\.0  |
| 13170070 | NaN                                                   | 9\.0   |
| 13170071 | NaN                                                   | 6\.0   |
| 13170072 | Still the easiest game to get to the table\. Es\.\.\. | 6\.0   |

<p>13170073 rows × 2 columns</p>

## Dropping rows with empty reviews

Great! We've got our dataset loaded.

Also, consecutive runs should be faster because we've pickled the data

As we can see, we've got several rows with no review and just a rating

We will be removing this rows as we cannot use them


```python
empty_reviews = reviews.comment.isna().sum()
print("Dropping {} rows from dataset".format(empty_reviews))

reviews.dropna(subset=['comment'], inplace=True)
reviews = reviews.reset_index(drop=True)

print("We now have {} reviews remaining".format(reviews.shape[0]))

gc.collect()
display(reviews)
```

    Dropping 10531901 rows from dataset
    We now have 2638172 reviews remaining


|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | http://boardgamers\.ro/saboteur\-recenzie/            | 6\.0   |
| 1       | A nice family game but a little light for my o\.\.\.  | 5\.0   |
| 2       | "This was the first modern board game that my \.\.\.  | 8\.0   |
| 3       | Great two player game\. In fact, how did Knizia\.\.\. | 9\.0   |
| 4       | Risk but with more pieces\.                           | 5\.0   |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | Great light game\. Theme and simplicity make it\.\.\. | 7\.0   |
| 2638168 | Detailed board  All sleeved                           | 9\.0   |
| 2638169 | "Solid deck\-builder in which you need to strat\.\.\. | 6\.5   |
| 2638170 | I adore this game\. Not only is it an incredibl\.\.\. | 10\.0  |
| 2638171 | Still the easiest game to get to the table\. Es\.\.\. | 6\.0   |

<p>2638172 rows × 2 columns</p>

## Cleaning the reviews

Wow! That was a huge difference. We've shed off about 10 million rows!

Now, we need to begin cleaning up these reviews as they have a lot of redundant information and context.

For this dataset, we will be removing the following patterns from the text:
- Words less than 3 and more than 15 characters
- Special characters
- URLs
- HTML tags
- All kinds of redundant whitespaces
- Words that have numbers between them
- Words that begin or end with a number

Once all this is done, we tokenize the reviews and save them in the dataframe


```python
def clean_text(text):
    text = text.lower().strip()
    text = " ".join([w for w in text.split() if len(w) >= 3 and len(w) <= 15])
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w+\d+\w*', '', text)
    text = re.sub('\d+\w+\d*', '', text)
    text = re.sub('\W+', '', text)
    text = tokenizer.tokenize(text)
    return text


if os.path.isfile(CLEAN_TEXT_PICKLE) or REVIEWS_DATA_CHANGED:
    print("Pickled clean text found. Loading from pickle file.")
    reviews = pd.read_pickle(CLEAN_TEXT_PICKLE, compression="gzip")
else:
    print("Cleaning reviews")
    tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
    reviews['comment'] = reviews['comment'].apply(clean_text)
    print("Pickling cleaned dataset")
    reviews.to_pickle(CLEAN_TEXT_PICKLE, compression="gzip")

gc.collect()
display(reviews)
```

    Pickled clean text found. Loading from pickle file.

|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | \[currently, this, sits, list, favorite, game\]       | 10\.0  |
| 1       | \[know, says, how, many, plays, but, many, many\.\.\. | 10\.0  |
| 2       | \[will, never, tire, this, game, awesome\]            | 10\.0  |
| 3       | \[this, probably, the, best, game, ever, played\.\.\. | 10\.0  |
| 4       | \[fantastic, game, got, hooked, games, all, ove\.\.\. | 10\.0  |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | \[horrible, party, game, dumping, this, one\]         | 3\.0   |
| 2638168 | \[difficult, build, anything, all, with, the, i\.\.\. | 3\.0   |
| 2638169 | \[lego, created, version, pictionary, only, you\.\.\. | 3\.0   |
| 2638170 | \[this, game, very, similar, creationary, comes\.\.\. | 2\.5   |
| 2638171 | \[this, game, was, really, bad, worst, that, pl\.\.\. | 2\.0   |

<p>2638172 rows × 2 columns</p>

## Calculating unique words

We've gotten rid of most of the oddities in our text data.

But, when it comes to such huge models text is present in many forms of speech so it's better to lemmatize them. But, before we do that, let's see how many unique words we currently have.


```python
uniq_words = []

unique_words = reviews.explode('comment').comment.nunique()
uniq_words.append(unique_words)

print("Unique words before lemmatizing {}".format(unique_words))
```

    Unique words before lemmatizing 313013


## Lemmatizing 

Wow, that's a lot of words. Let's see what happens after lemmatizing our reviews.


```python
def lemmatize_data(text):
    return [lemmatizer.lemmatize(w) for w in text]


if os.path.isfile(LEMMATIZED_PICKLE) or REVIEWS_DATA_CHANGED:
    print("Pickled lemmatized reviews found. Loading from pickle file.")
    reviews = pd.read_pickle(LEMMATIZED_PICKLE, compression="gzip")
else:
    print("Lemmatizing reviews")
    lemmatizer = nltk.stem.WordNetLemmatizer()
    reviews['comment'] = reviews['comment'].apply(lemmatize_data)
    print("Pickling lemmatized reviews")
    reviews.to_pickle(LEMMATIZED_PICKLE, compression="gzip")

display(reviews)
gc.collect()

unique_words = reviews.explode('comment').comment.nunique()
uniq_words.append(unique_words)
print("Unique words after lemmatizing {}".format(unique_words))
```

    Pickled lemmatized reviews found. Loading from pickle file.


|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | \[currently, this, sits, list, favorite, game\]       | 10\.0  |
| 1       | \[know, say, how, many, play, but, many, many, \.\.\. | 10\.0  |
| 2       | \[will, never, tire, this, game, awesome\]            | 10\.0  |
| 3       | \[this, probably, the, best, game, ever, played\.\.\. | 10\.0  |
| 4       | \[fantastic, game, got, hooked, game, all, over\.\.\. | 10\.0  |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | \[horrible, party, game, dumping, this, one\]         | 3\.0   |
| 2638168 | \[difficult, build, anything, all, with, the, i\.\.\. | 3\.0   |
| 2638169 | \[lego, created, version, pictionary, only, you\.\.\. | 3\.0   |
| 2638170 | \[this, game, very, similar, creationary, come,\.\.\. | 2\.5   |
| 2638171 | \[this, game, wa, really, bad, worst, that, pla\.\.\. | 2\.0   |

<p>2638172 rows × 2 columns</p>


    Unique words after lemmatizing 300678


## Removing stopwords and non-english words

Now let's remove the stopwords as well as they are mostly redundant for our training and mainly help us understand context.

We do have quite a fair number of custom words to remove and I've identified some custom words that I've added to the list as well.


```python
def remove_stopwords(text):
    words = [
        w for w in text if w not in stop_words and w in words_corpus or not w.isalpha()]
    words = list(filter(lambda word: len(word) > 2, set(words)))
    return words


if os.path.isfile(STOPWORDS_PICKLE) or REVIEWS_DATA_CHANGED:
    print("Reading stopwords pickle")
    reviews = pd.read_pickle(STOPWORDS_PICKLE, compression="gzip")
else:
    words_corpus = set(nltk.corpus.words.words())

    stop_words_json = {"en": ["a", "a's", "able", "about", "above", "according", "accordingly", "across", "actually", "after", "afterwards", "again", "against", "ain't", "aint", "all", "allow", "allows", "almost", "alone", "along", "already", "also", "although", "always", "am", "among", "amongst", "an", "and", "another", "any", "anybody", "anyhow", "anyone", "anything", "anyway", "anyways", "anywhere", "apart", "appear", "appreciate", "appropriate", "are", "aren't", "arent", "around", "as", "aside", "ask", "asking", "associated", "at", "available", "away", "awfully", "b", "be", "became", "because", "become", "becomes", "becoming", "been", "before", "beforehand", "behind", "being", "believe", "below", "beside", "besides", "best", "better", "between", "beyond", "both", "brief", "but", "by", "c", "c'mon", "cmon", "cs", "c's", "came", "can", "can't", "cannot", "cant", "cause", "causes", "certain", "certainly", "changes", "clearly", "co", "com", "come", "comes", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn't", "course", "currently", "d", "definitely", "described", "despite", "did", "didn't", "different", "do", "does", "doesn't", "doesn", "doing", "don't", "done", "down", "downwards", "during", "e", "each", "edu", "eg", "eight", "either", "else", "elsewhere", "enough", "entirely", "especially", "et", "etc", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere", "ex", "exactly", "example", "except", "f", "far", "few", "fifth", "first", "five", "followed", "following", "follows", "for", "former", "formerly", "forth", "four", "from", "further", "furthermore", "g", "get", "gets", "getting", "given", "gives", "go", "goes", "going", "gone", "got", "gotten", "greetings", "h", "had", "hadn't", "hadnt", "happens", "hardly", "has", "hasn't", "hasnt", "have", "haven't", "havent", "having", "he", "he's", "hes", "hello", "help", "hence", "her", "here", "here's", "heres", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "hi", "him", "himself", "his", "hither", "hopefully", "how", "howbeit", "however", "i", "i'd", "id", "i'll", "i'm", "im", "i've", "ive", "ie", "if", "ignored", "immediate", "in", "inasmuch", "inc", "indeed", "indicate", "indicated", "indicates", "inner", "insofar", "instead", "into", "inward", "is", "isn't", "isnt", "it", "it'd", "itd", "it'll", "itll", "it's", "its", "itself", "j", "just", "k", "keep", "keeps", "kept", "know", "known", "knows", "l", "last", "lately", "later", "latter", "latterly", "least", "less", "lest", "let", "let's", "lets", "like", "liked", "likely", "little", "look", "looking", "looks", "ltd", "m", "mainly", "many", "may", "maybe", "me", "mean", "meanwhile", "merely", "might", "more", "moreover", "most", "mostly", "much", "must", "my", "myself", "n", "name", "namely", "nd", "near", "nearly", "necessary", "need", "needs", "neither", "never", "nevertheless", "new", "next", "nine", "no", "nobody", "non", "none", "noone", "nor", "normally", "not", "nothing", "novel", "now", "nowhere", "o", "obviously", "of", "off", "often", "oh", "ok", "okay", "old", "on", "once", "one", "ones", "only", "onto", "or", "other", "others", "otherwise", "ought", "our", "ours", "ourselves", "out", "outside", "over", "overall", "own", "p", "particular", "particularly", "per", "perhaps", "placed", "please", "plus", "possible", "presumably", "probably", "provides", "q", "que", "quite", "qv", "r", "rather", "rd", "re", "really", "reasonably", "regarding", "regardless", "regards", "relatively", "respectively", "right", "s", "said", "same", "saw", "say", "saying", "says", "second", "secondly", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "shall", "she", "should", "shouldn't", "shouldnt", "since", "six", "so", "some", "somebody", "somehow", "someone", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "specified", "specify", "specifying", "still", "sub", "such", "sup", "sure", "t", "t's", "ts", "take", "taken", "tell", "tends", "th", "than", "thank", "thanks", "thanx", "that", "that's", "thats", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "there's", "theres", "thereafter", "thereby", "therefore", "therein", "theres", "thereupon", "these", "they", "they'd", "theyd", "they'll", "theyll", "they're", "theyre", "theyve", "they've", "think", "third", "this", "thorough", "thoroughly", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "took", "toward", "towards", "tried", "tries", "truly", "try", "trying", "twice", "two", "u", "un", "under", "unfortunately", "unless", "unlikely", "until", "unto", "up", "upon", "us", "use", "used", "useful", "uses", "using", "usually", "uucp", "v", "value", "various", "very", "via", "viz", "vs", "w", "wa", "want", "wants", "was", "wasn't", "wasnt", "way", "we", "we'd", "we'll", "we're", "we've", "weve", "welcome", "well", "went", "were", "weren't", "werent", "what", "what's", "whats", "whatever", "when", "whence", "whenever", "where", "wheres", "where's", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "who's", "whos", "whoever", "whole", "whom", "whose", "why", "will", "willing", "wish", "with", "within", "without", "won't", "wont", "wonder", "would", "wouldn't", "wouldny", "x", "y", "yes", "yet", "you", "you'd", "youd", "you'll", "youll", "you're", "youre", "you've", "youve", "your", "yours", "yours", "yourself", "yourselves", "z", "zero"]}

    stop_words_json_en = set(stop_words_json['en'])

    stop_words_nltk_en = set(stopwords.words('english'))

    custom_stop_words = ["rating", "ish", "havn", "dice", "end", "set", "doesnt", "give", "find", "doe", "system", "tile", "table", "deck", "box", "made", "part", "based", "worker", "wife", "put", "havent", "game", "play", "player", "one", "two", "card", "ha", "wa", "dont", "board", "time", "make", "rule", "thing", "version", "mechanic", "year", "theme", "rating", "family", "child", "money", "edition", "collection", "piece", "wasnt", "didnt"]

    stop_words = stop_words_nltk_en.union(
        stop_words_json_en, custom_stop_words)

    print("Removing {} stopwords from text".format(len(stop_words)))
    print()

    reviews['comment'] = reviews['comment'].apply(remove_stopwords)

    print("Pickling stopwords data")
    reviews.to_pickle(STOPWORDS_PICKLE, compression="gzip")

gc.collect()

print("After removing stopwords:")
display(reviews)
```

    Reading stopwords pickle
    After removing stopwords:



|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | \[list, favorite\]                                    | 10\.0  |
| 1       | \[uncounted\]                                         | 10\.0  |
| 2       | \[tire, awesome\]                                     | 10\.0  |
| 3       | \[negotiation, skill, thinking\]                      | 10\.0  |
| 4       | \[fantastic, hooked\]                                 | 10\.0  |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | \[party, horrible, dumping\]                          | 3\.0   |
| 2638168 | \[difficult, included, build\]                        | 3\.0   |
| 2638169 | \[create, abstract, fun, limited, number, pictu\.\.\. | 3\.0   |
| 2638170 | \[similar, creationary\]                              | 2\.5   |
| 2638171 | \[bad, worst, genre\]                                 | 2\.0   |

<p>2638172 rows × 2 columns</p>

Let's see what the word count is now


```python
unique_words = reviews.explode('comment').comment.nunique()
uniq_words.append(unique_words)
print(unique_words)
```

    37633


Wow! That a marked difference considering what we started off with. This will definitely help us during our training phase as it reduces training time and computations as well.

Moreover, this helps improve our accuracy as we will be targeting important keywords that contribute to a particular sentiment.

Let's see what the wordcloud looks like now.


```python
x = [1, 2, 3]
y = [313013, 300678, 37633]

fig, ax = plt.subplots(figsize=(10, 8))
ax.plot(x, y, color="#663399")
ax.set_ylabel('Unique Words')
ax.set_xlabel('Bins')
plt.show()
```


![png](rating_predictor_files/rating_predictor_21_0.png)


## Finishing up and saving progress

In the process of all this cleaning, there would be several rows that have become empty.

Let's remove them and then pickle our dataset so that we don't have to do all this cleaning again.


```python
def drop_empty_reviews(df):
    df = df.drop(df[~df.comment.astype(bool)].index)
    return df


if os.path.isfile(CLEANED_PICKLE) or REVIEWS_DATA_CHANGED:
    print("Reading cleaned reviews pickle")
    reviews = pd.read_pickle(CLEANED_PICKLE, compression="gzip")
else:
    reviews = drop_empty_reviews(reviews)
    print("Pickling cleaned reviews data")
    reviews.to_pickle(CLEANED_PICKLE, compression="gzip")

REVIEWS_DATA_CHANGED = False

gc.collect()

display(reviews)
```

    Reading cleaned reviews pickle



|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | \[list, favorite\]                                    | 10\.0  |
| 1       | \[uncounted\]                                         | 10\.0  |
| 2       | \[tire, awesome\]                                     | 10\.0  |
| 3       | \[negotiation, skill, thinking\]                      | 10\.0  |
| 4       | \[fantastic, hooked\]                                 | 10\.0  |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | \[party, horrible, dumping\]                          | 3\.0   |
| 2638168 | \[difficult, included, build\]                        | 3\.0   |
| 2638169 | \[create, abstract, fun, limited, number, pictu\.\.\. | 3\.0   |
| 2638170 | \[similar, creationary\]                              | 2\.5   |
| 2638171 | \[bad, worst, genre\]                                 | 2\.0   |

<p>2405729 rows × 2 columns</p>

## Exploring our dataset

Now that we've completed our cleaning, we need to assess what kind of model we should use to predict our ratings.

But first, let's see how many distinct values we have in the ratings column.


```python
reviews.rating.nunique()
```




    3213



We've got 3k+ unique values. This would definitely help us in a Regression model.

But, since we will be predicting single digit ratings, I'm going to round off the column. This will allow us to perform both regression and classification and choose what give's us a better result.

Do note that rounding off generally isn't recommended as it leads to loss of information and will affect the outcome.

I'm only doing this due to computational constraints. You should see better results in Regression models without rounding the values.


```python
reviews.rating = reviews.rating.round().astype(int)

reviews
```

|         | comment                                               | rating |
|---------|-------------------------------------------------------|--------|
| 0       | \[list, favorite\]                                    | 10     |
| 1       | \[uncounted\]                                         | 10     |
| 2       | \[tire, awesome\]                                     | 10     |
| 3       | \[negotiation, skill, thinking\]                      | 10     |
| 4       | \[fantastic, hooked\]                                 | 10     |
| \.\.\.  | \.\.\.                                                | \.\.\. |
| 2638167 | \[party, horrible, dumping\]                          | 3      |
| 2638168 | \[difficult, included, build\]                        | 3      |
| 2638169 | \[create, abstract, fun, limited, number, pictu\.\.\. | 3      |
| 2638170 | \[similar, creationary\]                              | 2      |
| 2638171 | \[bad, worst, genre\]                                 | 2      |

<p>2405729 rows × 2 columns</p>


We should have 11 distinct values (0 to 10) after this. Let's see how how many ratings of each category we have.


```python
reviews.rating.value_counts()
```




    8     595145
    7     520442
    6     480153
    9     219429
    5     200143
    10    140037
    4     127085
    3      66466
    2      38184
    1      18634
    0         11
    Name: rating, dtype: int64



We can see that we have less than 50 reviews with a zero rating. Their contribution is fairly negligible so we're going to remove them.


```python
class_distrib = reviews.rating.value_counts(ascending=True)
classes_to_drop = class_distrib[class_distrib < 50].index.values

for rating in classes_to_drop:
    rows_to_drop = reviews[reviews.rating == rating].index.to_list()
    reviews.drop(rows_to_drop, inplace=True)
    print("We have {} reviews remaining".format(reviews.shape))
```

    We have (2405718, 2) reviews remaining


Let's visualize the distribution of ratings a bit better by using a histogram


```python
fig, ax = plt.subplots(figsize=(10, 8))
sns.barplot(reviews.rating.value_counts().index,
            reviews.rating.value_counts(), color="#663399")
ax.set_xlabel("Rating")
ax.set_ylabel("Count")
fig.show()
```


![png](rating_predictor_files/rating_predictor_33_0.png)


## Splitting dataset for training, validation and testing

Looks like the data is following somewhat of a normal distribution. Most of the reviews lie between 6 to 9. This will definitely bias the model as the dataset is imbalanced.

Now that we have a vague idea about or course of action, let's split the dataset for training and testing. We will be using around 10% of this data mainly due to the volume and computational constraints.


```python
X_train, X_test, y_train, y_test = train_test_split(
    reviews.comment, reviews.rating, train_size=0.1, test_size=0.1)
X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, train_size=0.5)

X_train = X_train.apply(' '.join)
X_dev = X_dev.apply(' '.join)
X_test = X_test.apply(' '.join)

gc.collect()

print("Number of records chosen for training: {}".format(X_train.shape[0]))
print("Number of records chosen for development: {}".format(X_dev.shape[0]))
print("Number of records chosen for testing: {}".format(X_test.shape[0]))
```

    Number of records chosen for training: 240571
    Number of records chosen for development: 120286
    Number of records chosen for testing: 120286


Great! We've got that sorted. Since we have 10 unique values in ratings and we're dealing with text, let's start off with a Multinomial Naive Bayes Classifier.

But, before we do that, we need to calculate the TF-IDF values for our reviews. 

TF is the term-frequency for every word in the review i.e. the number of times a word has appeared in a review. This is fairly straightforward to calculate by using a counter per review.

IDF (Inverse Document Frequency) is slightly more trickier. Essentially, IDF is the weight of a word across all reviews. It is a measure of how common or rare a word is across the corpus. This helps us understand which words need to be prioritized over others. The closer IDF is to 0, the more common it is and the lesser it will contribute to the model. We need to target words that have a weight closer to 1 as they contribute the most.

This can be calculated by taking the total number of reviews, dividing it by the number of reviews that contain a word, and calculating the logarithm just to balance large numbers that can arise. This is what the final formula looks like:

$tfidf_{i,d} = tf_{i,d} \cdot idf_{i}$


```python
vectorizer = TfidfVectorizer()

X_train_vec = vectorizer.fit_transform(X_train)

X_dev_vec = vectorizer.transform(X_dev)
X_test_vec = vectorizer.transform(X_test)
```

## Selecting a model

We will be testing the following models
- Multinomial Naive Bayes (Works well with text data)
- Linear Regression (Target column is a number so maybe it will work)
- Logistic Regression (Works well with categorical data)

First let's give Naive Bayes a shot mainly due to the flexibility we get with text data. Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.

This is what the final classification rule looks like:

$\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y)$


```python
accuracy_log = []

mnbc = MultinomialNB()

mnbc.fit(X_train_vec, y_train)

mnbc_predicted = mnbc.predict(X_dev_vec)

acc_score = accuracy_score(mnbc_predicted, y_dev)
mnbc_accuracy = round(acc_score * 100, 2)
mnbc_report = classification_report(
    y_dev, mnbc_predicted, digits=4, zero_division=False)

gc.collect()

print("Accuracy for Multinomial Naive Bayes = {}%".format(mnbc_accuracy))
print()
accuracy_log.append(acc_score)
print("Classification Report for Multinomial Naive Bayes")
print(mnbc_report)
```

    Accuracy for Multinomial Naive Bayes = 29.4%
    
    Classification Report for Multinomial Naive Bayes
                  precision    recall  f1-score   support
    
               1     0.0000    0.0000    0.0000       926
               2     0.0000    0.0000    0.0000      1845
               3     0.0000    0.0000    0.0000      3357
               4     0.3286    0.0036    0.0071      6451
               5     0.1967    0.0047    0.0093     10112
               6     0.2816    0.3333    0.3053     24033
               7     0.2678    0.2212    0.2423     25999
               8     0.3073    0.7239    0.4314     29635
               9     0.2568    0.0035    0.0068     10955
              10     0.4318    0.0054    0.0108      6973
    
        accuracy                         0.2940    120286
       macro avg     0.2071    0.1296    0.1013    120286
    weighted avg     0.2724    0.2940    0.2221    120286
    



```python
print("Confusion Matrix for Multinomial Naive Bayes:")

mnb_cm = confusion_matrix(y_dev, mnbc_predicted, normalize='all')

fig, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(mnb_cm, annot=True, linewidths=0.5, ax=ax,
            cmap="Purples", linecolor="#663399", fmt="f", square=True)
fig.show()
```

    Confusion Matrix for Multinomial Naive Bayes:



![png](rating_predictor_files/rating_predictor_40_1.png)


Let's see how well our model works for a few real-world reviews


```python
def check_real_review_predictions(model, vectorizer):
    real_reviews = ["This game is absolutely pathetic. Horrible story and characters. Will never play this game again.",
                    "Worst.",
                    "One of the best games I've ever played. Amazing story and characters. Recommended.",
                    "good."]

    test_vectorizer = TfidfVectorizer(vocabulary=vectorizer.vocabulary_)
    vectorized_review = test_vectorizer.fit_transform(real_reviews)
    predicted = model.predict(vectorized_review)
    combined = np.vstack((real_reviews, predicted)).T
    return combined


preds = check_real_review_predictions(mnbc, vectorizer)
preds
```




    array([['This game is absolutely pathetic. Horrible story and characters. Will never play this game again.',
            '7'],
           ['Worst.', '6'],
           ["One of the best games I've ever played. Amazing story and characters. Recommended.",
            '8'],
           ['good.', '7']], dtype='<U97')



Well, this is exactly what we feared. All our ratings are between 6 to 8.

Looks like our model is being biased by our input data. I've got a feeling that some form of regression would work slighlty better here. Let's give Linear Regression a shot.


```python
linreg = LinearRegression(n_jobs=4)

linreg.fit(X_train_vec, y_train)

linreg_predicted = linreg.predict(X_dev_vec)

acc_score = accuracy_score(np.round(linreg_predicted), y_dev)
linreg_accuracy_round = round(acc_score * 100, 2)

gc.collect()

print("Accuracy for Linear Regression = {}%".format(linreg_accuracy_round))
accuracy_log.append(acc_score)
```

    Accuracy for Linear Regression = 27.47%


The accuracy is a little worse but let's check with some of our reviews before we dismiss this idea.


```python
preds = check_real_review_predictions(linreg, vectorizer)
preds
```




    array([['This game is absolutely pathetic. Horrible story and characters. Will never play this game again.',
            '4.437034389776194'],
           ['Worst.', '2.3588930267669355'],
           ["One of the best games I've ever played. Amazing story and characters. Recommended.",
            '9.024957161994099'],
           ['good.', '7.021012417756805']], dtype='<U97')



Perfect. Looks like we are on the right track. Since this is mainly a classification problem, Logistic Regression would be a better fit here. Let's try that:


```python
logreg = LogisticRegression(n_jobs=4)

logreg.fit(X_train_vec, y_train)

logreg_predicted = logreg.predict(X_dev_vec)

logreg_accuracy = round(accuracy_score(logreg_predicted, y_dev) * 100, 2)
logreg_report = classification_report(
    y_dev, logreg_predicted, digits=4, zero_division=False)

gc.collect()

print("Accuracy for Logistic Regression = {}%".format(logreg_accuracy))
print()
accuracy_log.append(acc_score)
print("Classification Report for Logistic Regression")
print(logreg_report)
```

    Accuracy for Logistic Regression = 29.94%
    
    Classification Report for Logistic Regression
                  precision    recall  f1-score   support
    
               1     0.2321    0.0594    0.0946       926
               2     0.2030    0.0580    0.0902      1845
               3     0.2054    0.0316    0.0547      3357
               4     0.2100    0.0632    0.0972      6451
               5     0.2145    0.0562    0.0890     10112
               6     0.2888    0.3937    0.3332     24033
               7     0.2679    0.2854    0.2764     25999
               8     0.3337    0.5489    0.4151     29635
               9     0.2703    0.0481    0.0817     10955
              10     0.3354    0.1565    0.2134      6973
    
        accuracy                         0.2994    120286
       macro avg     0.2561    0.1701    0.1745    120286
    weighted avg     0.2818    0.2994    0.2647    120286
    


While our acurracy is better, our solution hasn't fully converged. Let's increase the max iterations to a high value and see where it converges.

We can then use that and see how our model does. The default value is 100 so we will be increasing it to 1000.


```python
logreg = LogisticRegression(max_iter=1000, n_jobs=4)

logreg.fit(X_train_vec, y_train)

logreg_predicted = logreg.predict(X_dev_vec)

acc_score = accuracy_score(logreg_predicted, y_dev)
logreg_accuracy = round(acc_score * 100, 2)
logreg_report = classification_report(
    y_dev, logreg_predicted, digits=4, zero_division=False)

gc.collect()

print("Accuracy for Logistic Regression = {}%".format(logreg_accuracy))
print()
accuracy_log.append(acc_score)
print("Classification Report for Logistic Regression")
print(logreg_report)
print("Solution converged at iteration = {}".format(logreg.n_iter_[-1]))
```

    Accuracy for Logistic Regression = 30.08%
    
    Classification Report for Logistic Regression
                  precision    recall  f1-score   support
    
               1     0.3687    0.0713    0.1195       926
               2     0.2318    0.0569    0.0914      1845
               3     0.2272    0.0349    0.0604      3357
               4     0.2103    0.0629    0.0969      6451
               5     0.2022    0.0552    0.0867     10112
               6     0.2902    0.3916    0.3334     24033
               7     0.2717    0.2817    0.2766     25999
               8     0.3310    0.5616    0.4165     29635
               9     0.2674    0.0504    0.0848     10955
              10     0.3666    0.1428    0.2056      6973
    
        accuracy                         0.3008    120286
       macro avg     0.2767    0.1709    0.1772    120286
    weighted avg     0.2849    0.3008    0.2651    120286
    
    Solution converged at iteration = 637


Let's have a look at the confision matrix and see if things got any better


```python
print("Confusion Matrix for Logistic Regression:")

logreg_cm = confusion_matrix(y_dev, logreg_predicted, normalize='all')

fig, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(logreg_cm, annot=True, linewidths=0.5, ax=ax,
            cmap="Purples", linecolor="#663399", fmt="f", square=True)
fig.show()
```

    Confusion Matrix for Logistic Regression:



![png](rating_predictor_files/rating_predictor_52_1.png)


While we did lose some accuracy, our solution has converged and the confusion matrix looks slightly better. Let's see how it classifies our real world reviews.


```python
preds = check_real_review_predictions(logreg, vectorizer)
preds
```




    array([['This game is absolutely pathetic. Horrible story and characters. Will never play this game again.',
            '1'],
           ['Worst.', '1'],
           ["One of the best games I've ever played. Amazing story and characters. Recommended.",
            '10'],
           ['good.', '7']], dtype='<U97')



Perfect! These ratings correspond somewhat to what we were expecting.

But before we move ahead, we will ensure the consistency of these metrics by using Stratified K-Fold cross validation.

Since we are dealing with imbalanced data, a Stratified K-Fold is necessary to maintain the distribution of the input dataset.


```python
kfold = KFold(n_splits=3)
fold = 1

for train_index, test_index in kfold.split(X_train, y_train):
    X_tr, X_te = X_train[X_train.index.isin(
        train_index)], X_train[X_train.index.isin(test_index)]
    y_tr, y_te = y_train[y_train.index.isin(
        train_index)], y_train[y_train.index.isin(test_index)]

    vectorizer = TfidfVectorizer()

    X_tr_vec = vectorizer.fit_transform(X_tr)
    X_te_vec = vectorizer.transform(X_te)

    logreg.fit(X_tr_vec, y_tr)

    logreg_predicted = logreg.predict(X_te_vec)

    acc_score = accuracy_score(logreg_predicted, y_te)
    logreg_accuracy = round(acc_score * 100, 2)

    gc.collect()

    print("Accuracy for Logistic Regression Fold {} = {}%".format(
        fold, logreg_accuracy))
    print()
    fold += 1
```

    Accuracy for Logistic Regression Fold 1 = 30.94%
    
    Accuracy for Logistic Regression Fold 2 = 31.6%
    
    Accuracy for Logistic Regression Fold 3 = 28.53%
    


Looks like Logistic Regression is performaing fairly well when compared to it's Linear counterpart or even Naive Bayes for that matter. Let's see how the model is performing on the unseen test data.


```python
logreg.fit(X_train_vec, y_train)
final_predicted = logreg.predict(X_test_vec)

acc_score = accuracy_score(final_predicted, y_test)
logreg_accuracy = round(acc_score * 100, 2)

print("Final Accuracy for test data with Logistic Regression = {}%".format(
    logreg_accuracy))
```

    Final Accuracy for test data with Logistic Regression = 30.16%


While this isn't the accuracy we were expecting it looks like our accuracy is platueauing. This could be due to the inherent bias in the dataset.

Due to this, I will be training the model using a around 50% of the data and pickling it so that we can use it in our online Flask application to predict any new reviews that a user can input.


```python
X_train, _, y_train, _ = train_test_split(
    reviews.comment, reviews.rating, train_size=0.5, test_size=0.01)
X_train = X_train.apply(' '.join)

print("Number of records chosen for training the final model = {}".format(
    X_train.shape[0]))

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)

logreg.fit(X_train_vec, y_train)
gc.collect()
```

    Number of records chosen for training the final model = 1202859





    42



We will pickle this model and the vocabulary of the TF-IDF vectorizer. This will be used in the Flask application that we will be building.


```python
if os.path.isfile(MODEL_PICKLE) or REVIEWS_DATA_CHANGED:
    print('Pickled model already present')
    os.remove(DATA_DIR + MODEL_PICKLE)

if os.path.isfile(VOCABULARY_PICKLE) or REVIEWS_DATA_CHANGED:
    print('Pickled vocab already present')
    os.remove(DATA_DIR + VOCABULARY_PICKLE)

print("Pickling model")
pickle.dump(logreg, open(MODEL_PICKLE, "wb"))

print("Pickling vocabulary")
pickle.dump(vectorizer.vocabulary_, open(VOCABULARY_PICKLE, "wb"))
```

    Pickling model
    Pickling vocabulary


## Challenges faced

The biggest challenges that I faced revolved around cleaning the dataset and dealing with imbalanced data.

Every model that I tested was giving similar accuracy thus leading to the conclusion that the dataset itself was creating an implict bias.

The dataset was difficult to clean and implement models on due to the sheer volume of data and the hardware constraints.

Even while attempting to deal with imbalanced data, there was issues with accuracy because methods such as SMOTE and StratifiedKFold do not work well with text data as the features.

The other challenge I faced was deploying the model to a production environment. If I used a majority of the data to train the model, it became too heavy and every AJAX call to the server took a long time to load.

Thus, I ended up loading the model as soon as the server started and caching it in the memory thereby speeding up consecutive requests.


## My contribution

Built a Flask application that predicts the rating of any review. Please visit the page [here](https://karanr.dev/rating-predictor) and enter a review. You should see the prediction for that review. This application has been deployed on Heroku and the form is available on my Portfolio website that is built using GatsbyJS. The code for this application is available on my [GitHub](https://github.com/karanrajpal14/rating-predictor).

You can also see a demo of this application on [YouTube](https://www.youtube.com/watch?v=97LN1FVY6gU).

A copy of the dataset and it's pickle files are available [here](https://drive.google.com/open?id=1uW3E6lwW0A9ay30Q6LLcoUAHYBE9pQUk) and the IPython Notebook can be downloaded from [here](https://www.karanr.dev/rating_predictor.ipynb). **(Right-click and Save As to download the file)**


## References

[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)

[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic#sklearn.linear_model.LogisticRegression)

[Sending POST Request via Axios](https://stackoverflow.com/questions/57014579/getting-none-values-for-post-request-via-the-axios-library-sent-to-python-djan)

[SMOTE Oversampling](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)

[Regression Methods](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)

[Pickling ML models](https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)

[Multi-class Text Classification](https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f)

[Multinomial Naive Bayes](https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b)

[TF-IDF Vectorizer](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/)